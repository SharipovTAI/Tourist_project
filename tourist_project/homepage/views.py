from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
import json
import traceback
from llama_cpp import Llama
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import re

# -----------------------------------------
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ LLaMA
# -----------------------------------------
try:
    llm = Llama(
        model_path="D:/project_workshop/Llama_project/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        n_ctx=4096,
        n_gpu_layers=20,
        verbose=False
    )
    MODEL_LOADED = True
    print("‚úì –ú–æ–¥–µ–ª—å LLaMA –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
except Exception as e:
    MODEL_LOADED = False
    print("‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ LLaMA:", e)
    print(traceback.format_exc())

# -----------------------------------------
# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π
# -----------------------------------------
TEXT_PATH = "D:\\project_workshop\\tourist_project\\homepage\\zapovedniki_optimized_grouped.txt"

# –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
zapovedniki_data = {}
all_paragraphs = []
paragraph_to_zapovednik = {}

try:
    if os.path.exists(TEXT_PATH):
        with open(TEXT_PATH, 'r', encoding='utf-8') as f:
            content = f.read()
        
        current_zapovednik = None
        for line in content.split('\n'):
            line = line.strip()
            if not line:
                continue
            
            if line.startswith('===') and '===' in line:
                match = re.match(r'=== (.*?) ===', line)
                if match:
                    current_zapovednik = match.group(1).strip()
                    zapovedniki_data[current_zapovednik] = []
                continue
            
            if current_zapovednik and line:
                zapovedniki_data[current_zapovednik].append(line)
                all_paragraphs.append(line)
                paragraph_to_zapovednik[len(all_paragraphs)-1] = current_zapovednik
        
        print("="*60)
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(zapovedniki_data)} –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–æ–≤")
        print(f"‚úì –í—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(all_paragraphs)}")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¢–∞–π–º—ã—Ä—É
        if "–¢–ê–ô–ú–´–†–°–ö–ò–ô –ó–ê–ü–û–í–ï–î–ù–ò–ö" in zapovedniki_data:
            taiymir_count = len(zapovedniki_data["–¢–ê–ô–ú–´–†–°–ö–ò–ô –ó–ê–ü–û–í–ï–î–ù–ò–ö"])
            geopolit_count = sum(1 for s in zapovedniki_data["–¢–ê–ô–ú–´–†–°–ö–ò–ô –ó–ê–ü–û–í–ï–î–ù–ò–ö"] 
                               if any(word in s.lower() for word in ['–≥–µ–æ–ø–æ–ª–∏—Ç', '—Å—É–≤–µ—Ä–µ–Ω–∏—Ç–µ—Ç', '—Ä–µ—Å—É—Ä—Å', '–∞—Ä–∫—Ç–∏–∫', '—Å–º–ø', '—Å–µ–≤–µ—Ä–Ω—ã–π']))
            print(f"‚úì –¢–∞–π–º—ã—Ä—Å–∫–∏–π –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫: {taiymir_count} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∏–∑ –Ω–∏—Ö {geopolit_count} –ø—Ä–æ –≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫—É")
        
    else:
        print(f"‚úó –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {TEXT_PATH}")
        all_paragraphs = []
        
except Exception as e:
    all_paragraphs = []
    print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")

# -----------------------------------------
# –ó–∞–≥—Ä—É–∂–∞–µ–º SentenceTransformer
# -----------------------------------------
try:
    embed_model = SentenceTransformer('all-MiniLM-L6-v2')
    if all_paragraphs:
        print("–°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        doc_embs = embed_model.encode(all_paragraphs, convert_to_numpy=True)
        EMBEDDINGS_LOADED = True
        print(f"‚úì –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã: {doc_embs.shape}")
    else:
        EMBEDDINGS_LOADED = False
        doc_embs = None
        print("‚ö† –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
except Exception as e:
    EMBEDDINGS_LOADED = False
    doc_embs = None
    print(f"‚úó –û—à–∏–±–∫–∞ SentenceTransformer: {e}")

# -----------------------------------------
# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–æ–≤
# -----------------------------------------
ZAPOVEDNIKI_KEYWORDS = {
    '—Ç–∞–π–º—ã—Ä—Å–∫–∏–π': '–¢–ê–ô–ú–´–†–°–ö–ò–ô –ó–ê–ü–û–í–ï–î–ù–ò–ö',
    '—Ç–∞–π–º—ã—Ä': '–¢–ê–ô–ú–´–†–°–ö–ò–ô –ó–ê–ü–û–í–ï–î–ù–ò–ö',
    # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–∏
}

def detect_zapovednik(question):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ–º, –æ –∫–∞–∫–æ–º –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–µ –∏–¥–µ—Ç —Ä–µ—á—å"""
    question_lower = question.lower()
    
    for keyword, zapovednik_name in ZAPOVEDNIKI_KEYWORDS.items():
        if keyword in question_lower:
            return zapovednik_name
    
    for zapovednik_name in zapovedniki_data.keys():
        main_word = zapovednik_name.split()[0].lower()
        if main_word in question_lower:
            return zapovednik_name
    
    return None

# -----------------------------------------
# –£–õ–£–ß–®–ï–ù–ù–´–ô –ø–æ–∏—Å–∫ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
# -----------------------------------------
def get_context_for_question(question, zapovednik_name=None):
    """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    if not EMBEDDINGS_LOADED or doc_embs is None:
        return []
    
    try:
        print(f"\nüîç –ü–û–ò–°–ö: '{question}'")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —Å–ª–æ–∂–Ω—ã–π –ª–∏ –≤–æ–ø—Ä–æ—Å
        is_complex = any(word in question.lower() for word in ['–∏ ', '–∞ —Ç–∞–∫–∂–µ', '—Ç–∞–∫–∂–µ', '–ø–ª—é—Å', '–≤–∫–ª—é—á–∞—è', '–∫—Ä–æ–º–µ'])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—ã –≤–æ–ø—Ä–æ—Å–∞
        question_lower = question.lower()
        themes = []
        
        if '–≥–µ–æ–ø–æ–ª–∏—Ç' in question_lower or '–≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π' in question_lower:
            themes.append('–≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞')
        
        # –£–ª—É—á—à–∞–µ–º –≤–æ–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞
        enhanced_question = question_lower
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–º
        if '–≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞' in themes:
            enhanced_question += " —Å—É–≤–µ—Ä–µ–Ω–∏—Ç–µ—Ç —Ä–µ—Å—É—Ä—Å—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –∞—Ä–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–µ–≤–µ—Ä–Ω—ã–π –º–æ—Ä—Å–∫–æ–π –ø—É—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è –≥—Ä–∞–Ω–∏—Ü–∞"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–æ–π –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫ –∏—â–µ–º
        if not zapovednik_name:
            zapovednik_name = detect_zapovednik(question)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫—É
        if zapovednik_name and zapovednik_name in zapovedniki_data:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —ç—Ç–æ–≥–æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–∞
            target_indices = []
            for idx, zap in paragraph_to_zapovednik.items():
                if zap == zapovednik_name:
                    target_indices.append(idx)
            
            if not target_indices:
                return []
            
            # –ë–µ—Ä–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–ª—å–∫–æ —ç—Ç–æ–≥–æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–∞
            target_embs = doc_embs[target_indices]
            target_paragraphs = [all_paragraphs[i] for i in target_indices]
        else:
            # –ò—â–µ–º –ø–æ –≤—Å–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
            target_indices = list(range(len(all_paragraphs)))
            target_embs = doc_embs
            target_paragraphs = all_paragraphs
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        question_emb = embed_model.encode([enhanced_question], convert_to_numpy=True)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = np.dot(target_embs, question_emb.T).flatten()
        
        # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –±–µ—Ä–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_n = 25 if is_complex else 15
        top_indices_all = np.argsort(similarities)[::-1][:top_n]
        
        # –ü–ï–†–í–´–ô –ü–†–û–•–û–î: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        semantic_results = []
        for idx in top_indices_all:
            if similarities[idx] > 0.15:  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                semantic_results.append(target_paragraphs[idx])
        
        # –í–¢–û–†–û–ô –ü–†–û–•–û–î: –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Å–ª–æ–∂–Ω—ã–π –∏/–∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–º—ã, –∏—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keyword_results = []
        if is_complex or themes:
            print(f"  –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–º–∞–º: {themes}")
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
            theme_keywords = {
                '–≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞': ['–≥–µ–æ–ø–æ–ª–∏—Ç', '—Å—É–≤–µ—Ä–µ–Ω–∏—Ç–µ—Ç', '—Ä–µ—Å—É—Ä—Å', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫', '–∞—Ä–∫—Ç–∏—á–µ—Å–∫', '—Å–µ–≤–µ—Ä–Ω—ã–π –º–æ—Ä—Å–∫–æ–π', '—Å–º–ø', '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç', '—Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏', '–≥—Ä–∞–Ω–∏—Ü']
            }
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            all_keywords = []
            for theme in themes:
                if theme in theme_keywords:
                    all_keywords.extend(theme_keywords[theme])
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∏—â–µ–º –ø–æ –Ω–∏–º
            if all_keywords:
                for paragraph in target_paragraphs:
                    paragraph_lower = paragraph.lower()
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                    if any(keyword in paragraph_lower for keyword in all_keywords):
                        if paragraph not in semantic_results and paragraph not in keyword_results:
                            keyword_results.append(paragraph)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_results = semantic_results.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏)
        for result in keyword_results:
            if result not in all_results:
                all_results.append(result)
        
        # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –±–µ—Ä–µ–º –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        max_results = 20 if is_complex else 12
        
        print(f"  –ù–∞–π–¥–µ–Ω–æ: {len(semantic_results)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö + {len(keyword_results)} –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º = {len(all_results)} –≤—Å–µ–≥–æ")
        
        if all_results:
            print("  –ü—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:")
            for i, text in enumerate(all_results[:3]):
                print(f"    {i+1}. {text}")
        
        return all_results[:max_results]
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        return []

# -----------------------------------------
# –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
# -----------------------------------------
def index(request):
    return render(request, 'homepage/homepage.html')

# -----------------------------------------
# –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô API –º–µ—Ç–æ–¥
# -----------------------------------------
@csrf_exempt
def ask_llama(request):
    if not MODEL_LOADED:
        return JsonResponse({"error": "–ú–æ–¥–µ–ª—å LLaMA –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}, status=500)
    
    if not EMBEDDINGS_LOADED:
        return JsonResponse({"error": "–°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}, status=500)
    
    if not all_paragraphs:
        return JsonResponse({"error": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞"}, status=500)

    if request.method != "POST":
        return JsonResponse({"error": "–ú–µ—Ç–æ–¥ –Ω–µ —Ä–∞–∑—Ä–µ—à–µ–Ω"}, status=405)

    try:
        data = json.loads(request.body)
        user_question = data.get("question", "").strip()

        if not user_question:
            return JsonResponse({"error": "–í–æ–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º"}, status=400)

        print(f"\n{'='*60}")
        print(f"üì® –í–û–ü–†–û–°: '{user_question}'")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫
        detected_zapovednik = detect_zapovednik(user_question)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
        context_paragraphs = get_context_for_question(user_question, detected_zapovednik)
        
        if context_paragraphs:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤—Å–µ–º —á–∞—Å—Ç—è–º –≤–æ–ø—Ä–æ—Å–∞
            question_lower = user_question.lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            if '–≥–µ–æ–ø–æ–ª–∏—Ç' in question_lower:
                geopolit_in_context = any(
                    any(word in p.lower() for word in ['–≥–µ–æ–ø–æ–ª–∏—Ç', '—Å—É–≤–µ—Ä–µ–Ω–∏—Ç–µ—Ç', '—Ä–µ—Å—É—Ä—Å', '–∞—Ä–∫—Ç–∏—á–µ—Å–∫', '—Å–µ–≤–µ—Ä–Ω—ã–π –º–æ—Ä—Å–∫–æ–π', '—Å–º–ø'])
                    for p in context_paragraphs
                )
                print(f"  –ì–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: {'–î–ê' if geopolit_in_context else '–ù–ï–¢'}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π
            context_text = "–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ó–ê–ü–û–í–ï–î–ù–ò–ö–ê–• –†–û–°–°–ò–ò:\n\n"
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–∞–º
            context_by_zapovednik = {}
            for text in context_paragraphs:
                for zap_name, sentences in zapovedniki_data.items():
                    if text in sentences:
                        if zap_name not in context_by_zapovednik:
                            context_by_zapovednik[zap_name] = []
                        context_by_zapovednik[zap_name].append(text)
                        break
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–∞–º
            for zap_name, sentences in context_by_zapovednik.items():
                context_text += f"=== {zap_name} ===\n"
                for sentence in sentences:
                    context_text += f"- {sentence}\n"
                context_text += "\n"
            
            print(f"üìö –ö–æ–Ω—Ç–µ–∫—Å—Ç: {len(context_paragraphs)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ {len(context_by_zapovednik)} –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–æ–≤")
        else:
            context_text = "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
            print("‚ö† –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –£–°–ò–õ–ï–ù–ù–´–ô –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        if '–∏' in user_question.lower() and user_question.lower().count('–∏') > 1:
            # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏
            prompt = f"""<|im_start|>system
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–∞–º –†–æ—Å—Å–∏–∏. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ü–û–õ–ù–û–°–¢–¨–Æ, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å—é –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–í–ê–ñ–ù–û: –í–æ–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π. –û—Ç–≤–µ—Ç—å –Ω–∞ –í–°–ï —á–∞—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞.

–ò–ù–§–û–†–ú–ê–¶–ò–Ø:
{context_text}

–ü–†–ê–í–ò–õ–ê –û–¢–í–ï–¢–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π –í–°–Æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
2. –û—Ç–≤–µ—Ç—å –Ω–∞ –ö–ê–ñ–î–£–Æ —á–∞—Å—Ç—å –≤–æ–ø—Ä–æ—Å–∞
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∫–æ–π-—Ç–æ —á–∞—Å—Ç–∏ –Ω–µ—Ç, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –ø—Ä—è–º–æ
4. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã
5. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ø–æ —á–∞—Å—Ç—è–º –≤–æ–ø—Ä–æ—Å–∞<|im_end|>
<|im_start|>user
{user_question}<|im_end|>
<|im_start|>assistant
"""
        else:
            # –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            prompt = f"""<|im_start|>system
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–∞–º –†–æ—Å—Å–∏–∏. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–ò–ù–§–û–†–ú–ê–¶–ò–Ø:
{context_text}

–ü–†–ê–í–ò–õ–ê –û–¢–í–ï–¢–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
3. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã
4. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º<|im_end|>
<|im_start|>user
{user_question}<|im_end|>
<|im_start|>assistant
"""
        
        print(f"üìù –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        response = llm(
            prompt,
            max_tokens=600,  # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            temperature=0.1,
            top_p=0.85,
            repeat_penalty=1.1,
            stop=["<|im_start|>", "<|im_end|>", "<|eot_id|>"],
            echo=False
        )

        answer = response["choices"][0]["text"].strip()
        
        # –ß–∏—Å—Ç–∫–∞
        for token in ["<|im_start|>", "<|im_end|>", "<|eot_id|>"]:
            answer = answer.replace(token, "").strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ—Ç–≤–µ—Ç–∏–ª –ª–∏ –Ω–∞ –≤—Å–µ —á–∞—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞
        question_lower = user_question.lower()
        answer_lower = answer.lower()
        
        # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å "–∏" –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤—Å–µ–º —á–∞—Å—Ç—è–º
        if '–∏' in question_lower and '–≥–µ–æ–ø–æ–ª–∏—Ç' in question_lower:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –æ—Ç–≤–µ—Ç–µ –≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            geopolit_keywords = ['–≥–µ–æ–ø–æ–ª–∏—Ç', '—Å—É–≤–µ—Ä–µ–Ω–∏—Ç–µ—Ç', '—Ä–µ—Å—É—Ä—Å', '–∞—Ä–∫—Ç–∏—á–µ—Å–∫', '—Å–µ–≤–µ—Ä–Ω—ã–π –º–æ—Ä—Å–∫–æ–π', '—Å–º–ø']
            has_geopolit = any(keyword in answer_lower for keyword in geopolit_keywords)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–µ
            has_general = any(word in answer_lower for word in ['–∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫', '—Ç–∞–π–º—ã—Ä', '–æ—Å–Ω–æ–≤–∞–Ω', '–ø–ª–æ—â–∞–¥'])
            
            if not has_geopolit and has_general:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                geopolit_context = []
                for text in context_paragraphs:
                    if any(keyword in text.lower() for keyword in geopolit_keywords):
                        geopolit_context.append(text)
                
                if geopolit_context:
                    answer += "\n\n–ì–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã:\n" + "\n".join([f"‚Ä¢ {text}" for text in geopolit_context[:3]])
                else:
                    answer += "\n\n–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –æ –≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–∞—Ö –¢–∞–π–º—ã—Ä–∞."
        
        # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
        if len(answer) < 30:
            answer = "–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:\n" + "\n".join([f"‚Ä¢ {p}" for p in context_paragraphs[:5]])
        
        print(f"üì§ –û–¢–í–ï–¢ ({len(answer)} —Å–∏–º–≤–æ–ª–æ–≤):")
        print(f"{answer[:300]}..." if len(answer) > 300 else answer)
        print(f"{'='*60}")
        
        return JsonResponse({
            "answer": answer,
            "question": user_question,
            "zapovednik": detected_zapovednik if detected_zapovednik else "–û–±—â–∏–π –≤–æ–ø—Ä–æ—Å",
            "context_count": len(context_paragraphs),
            "complex_question": '–∏' in user_question.lower() and user_question.lower().count('–∏') > 0
        })

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {traceback.format_exc()}")
        return JsonResponse({"error": str(e)}, status=500)

# -----------------------------------------
# –î–ï–ë–ê–ì-–º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∏—Å–∫–∞
# -----------------------------------------
@csrf_exempt
def debug_search(request):
    """–ú–µ—Ç–æ–¥ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –ø–æ–∏—Å–∫–∞"""
    if request.method != "POST":
        return JsonResponse({"error": "–ú–µ—Ç–æ–¥ –Ω–µ —Ä–∞–∑—Ä–µ—à–µ–Ω"}, status=405)
    
    try:
        data = json.loads(request.body)
        user_question = data.get("question", "").strip()
        
        if not user_question:
            return JsonResponse({"error": "–í–æ–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º"}, status=400)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫
        detected_zapovednik = detect_zapovednik(user_question)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_paragraphs = get_context_for_question(user_question, detected_zapovednik)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        analysis = {
            "question": user_question,
            "detected_zapovednik": detected_zapovednik,
            "total_context_count": len(context_paragraphs),
            "context_by_zapovednik": {},
            "has_geopolit": False,
            "has_general": False
        }
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫–∞–º
        for text in context_paragraphs:
            for zap_name, sentences in zapovedniki_data.items():
                if text in sentences:
                    if zap_name not in analysis["context_by_zapovednik"]:
                        analysis["context_by_zapovednik"][zap_name] = []
                    analysis["context_by_zapovednik"][zap_name].append(text)
                    break
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        geopolit_keywords = ['–≥–µ–æ–ø–æ–ª–∏—Ç', '—Å—É–≤–µ—Ä–µ–Ω–∏—Ç–µ—Ç', '—Ä–µ—Å—É—Ä—Å', '–∞—Ä–∫—Ç–∏—á–µ—Å–∫', '—Å–µ–≤–µ—Ä–Ω—ã–π –º–æ—Ä—Å–∫–æ–π', '—Å–º–ø']
        for text in context_paragraphs:
            if any(keyword in text.lower() for keyword in geopolit_keywords):
                analysis["has_geopolit"] = True
                break
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        general_keywords = ['–∑–∞–ø–æ–≤–µ–¥–Ω–∏–∫', '–æ—Å–Ω–æ–≤–∞–Ω', '–ø–ª–æ—â–∞–¥', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω']
        for text in context_paragraphs:
            if any(keyword in text.lower() for keyword in general_keywords):
                analysis["has_general"] = True
                break
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        return JsonResponse({
            "analysis": analysis,
            "context_preview": context_paragraphs[:10],
            "all_context": context_paragraphs
        })
        
    except Exception as e:
        return JsonResponse({"error": str(e)}, status=500)
